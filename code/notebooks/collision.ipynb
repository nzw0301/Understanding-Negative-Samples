{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import yaml\n",
    "from sklearn.preprocessing import normalize\n",
    "from numba import jit\n",
    "\n",
    "\n",
    "from utils import get_weight_path_in_current_system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features() -> dict:\n",
    "    datasets = (\"cifar10\", \"cifar100\", \"ag_news\")\n",
    "    epochs = (500, 500, 100)\n",
    "\n",
    "    features = {}\n",
    "    for dataset, epoch in zip(datasets, epochs):\n",
    "\n",
    "        base_dir = pathlib.Path(\"../results/{}/analysis/save_unnormalised_feature/\".format(dataset))\n",
    "\n",
    "        for config_path in base_dir.glob(\"**/config.yaml\"):\n",
    "                \n",
    "            with open(config_path) as f:\n",
    "                config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "                \n",
    "                seed = config[\"experiment\"][\"seed\"]\n",
    "\n",
    "                if config[\"experiment\"][\"use_projection_head\"]:\n",
    "                    extractor = \"Head\"\n",
    "                else:\n",
    "                    extractor = \"Without Head\"\n",
    "\n",
    "            self_sup_path = pathlib.Path(\n",
    "                get_weight_path_in_current_system(config[\"experiment\"][\"target_weight_file\"])).parent\n",
    "\n",
    "            with open(self_sup_path / \".hydra\" / \"config.yaml\") as f:\n",
    "                config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "                num_mini_batches = config[\"experiment\"][\"batches\"]\n",
    "\n",
    "\n",
    "            path = config_path.parent.parent\n",
    "\n",
    "            d = dataset.replace(\"100\", \"\").replace(\"10\", \"\")\n",
    "            y_train = np.load(path / \"epoch_{}-{}.pt.label.train.npy\".format(epoch, d))\n",
    "\n",
    "            X_train_0 = np.load(path / \"epoch_{}-{}.pt.feature.0.train.npy\".format(epoch, d))\n",
    "            X_train_1 = np.load(path / \"epoch_{}-{}.pt.feature.1.train.npy\".format(epoch, d))\n",
    "            \n",
    "            d_name = dataset\n",
    "\n",
    "            if \"augmentation_type\" in config[\"dataset\"]:\n",
    "                d_name = \"{}-{}\".format(dataset, config[\"dataset\"][\"augmentation_type\"])\n",
    "                \n",
    "            if d_name not in features:\n",
    "                features[d_name] = {}                \n",
    "\n",
    "            if extractor not in features[d_name]:\n",
    "                features[d_name][extractor] = {}\n",
    "\n",
    "            if seed not in features[d_name][extractor]:\n",
    "                features[d_name][extractor][seed] = {}\n",
    "\n",
    "            features[d_name][extractor][seed][num_mini_batches] = (\n",
    "                X_train_0,\n",
    "                X_train_1,\n",
    "                y_train\n",
    "            )\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True, parallel=True)\n",
    "def compute_bound(c, y_train, X_train_0, X_train_1):\n",
    "    target_ids = y_train == c\n",
    "    X_train_0_c = X_train_0[target_ids]\n",
    "    X_train_1_c = X_train_1[target_ids]\n",
    "    cos_sim = X_train_0_c.dot(X_train_1_c.T)\n",
    "    n = np.sum(target_ids)\n",
    "\n",
    "    bounds_by_sample = np.abs(cos_sim - np.diag(cos_sim)).sum(axis=0) / (n - 1)\n",
    "    return bounds_by_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar10 Without Head 13 32 0.20344003\n",
      "cifar10 Without Head 13 64 0.20893289\n",
      "cifar10 Without Head 13 128 0.21950303\n",
      "cifar10 Without Head 13 256 0.23831828\n",
      "cifar10 Without Head 13 512 0.37764415\n",
      "cifar10 Without Head 11 32 0.20120734\n",
      "cifar10 Without Head 11 64 0.21036161\n",
      "cifar10 Without Head 11 128 0.21415293\n",
      "cifar10 Without Head 11 256 0.24843346\n",
      "cifar10 Without Head 11 512 0.3816658\n",
      "cifar10 Without Head 7 32 0.2020581\n",
      "cifar10 Without Head 7 64 0.20859843\n",
      "cifar10 Without Head 7 128 0.21780524\n",
      "cifar10 Without Head 7 256 0.24569704\n",
      "cifar10 Without Head 7 512 0.37573105\n",
      "cifar10 Head 13 32 0.6004361\n",
      "cifar10 Head 13 64 0.60644984\n",
      "cifar10 Head 13 128 0.6113785\n",
      "cifar10 Head 13 256 0.6158205\n",
      "cifar10 Head 13 512 0.62248313\n",
      "cifar10 Head 11 32 0.60062814\n",
      "cifar10 Head 11 64 0.6057505\n",
      "cifar10 Head 11 128 0.60984033\n",
      "cifar10 Head 11 256 0.6150309\n",
      "cifar10 Head 11 512 0.6226506\n",
      "cifar10 Head 7 32 0.60107994\n",
      "cifar10 Head 7 64 0.60679954\n",
      "cifar10 Head 7 128 0.6118673\n",
      "cifar10 Head 7 256 0.6146793\n",
      "cifar10 Head 7 512 0.6232999\n",
      "cifar100 Head 13 128 0.5233955\n",
      "cifar100 Head 13 256 0.5195236\n",
      "cifar100 Head 13 384 0.5152851\n",
      "cifar100 Head 13 512 0.5130686\n",
      "cifar100 Head 13 640 0.51149356\n",
      "cifar100 Head 13 768 0.5096279\n",
      "cifar100 Head 13 896 0.5117583\n",
      "cifar100 Head 13 1024 0.5077058\n",
      "cifar100 Head 11 128 0.5218632\n",
      "cifar100 Head 11 256 0.5186\n",
      "cifar100 Head 11 384 0.51519334\n",
      "cifar100 Head 11 512 0.51290786\n",
      "cifar100 Head 11 640 0.51123\n",
      "cifar100 Head 11 768 0.5095178\n",
      "cifar100 Head 11 896 0.5113434\n",
      "cifar100 Head 11 1024 0.50731766\n",
      "cifar100 Head 7 128 0.524001\n",
      "cifar100 Head 7 256 0.5196578\n",
      "cifar100 Head 7 384 0.516787\n",
      "cifar100 Head 7 512 0.51445174\n",
      "cifar100 Head 7 640 0.5122713\n",
      "cifar100 Head 7 768 0.5109851\n",
      "cifar100 Head 7 896 0.5112811\n",
      "cifar100 Head 7 1024 0.508517\n",
      "cifar100 Without Head 13 128 0.21599847\n",
      "cifar100 Without Head 13 256 0.23525405\n",
      "cifar100 Without Head 13 384 0.29480755\n",
      "cifar100 Without Head 13 512 0.33783492\n",
      "cifar100 Without Head 13 640 0.35608903\n",
      "cifar100 Without Head 13 768 0.37021708\n",
      "cifar100 Without Head 13 896 0.37402523\n",
      "cifar100 Without Head 13 1024 0.38472536\n",
      "cifar100 Without Head 11 128 0.21888873\n",
      "cifar100 Without Head 11 256 0.23722278\n",
      "cifar100 Without Head 11 384 0.29888344\n",
      "cifar100 Without Head 11 512 0.34190804\n",
      "cifar100 Without Head 11 640 0.3611424\n",
      "cifar100 Without Head 11 768 0.37094355\n",
      "cifar100 Without Head 11 896 0.3727542\n",
      "cifar100 Without Head 11 1024 0.38792542\n",
      "cifar100 Without Head 7 128 0.21592535\n",
      "cifar100 Without Head 7 256 0.23009011\n",
      "cifar100 Without Head 7 384 0.29627556\n",
      "cifar100 Without Head 7 512 0.33912125\n",
      "cifar100 Without Head 7 640 0.35595816\n",
      "cifar100 Without Head 7 768 0.3714959\n",
      "cifar100 Without Head 7 896 0.37216112\n",
      "cifar100 Without Head 7 1024 0.3892289\n",
      "ag_news-erase Without Head 13 16 0.6802308\n",
      "ag_news-erase Without Head 13 32 0.6587549\n",
      "ag_news-erase Without Head 13 64 0.63992685\n",
      "ag_news-erase Without Head 13 128 0.65759057\n",
      "ag_news-erase Without Head 13 256 0.64326227\n",
      "ag_news-erase Without Head 13 512 0.6576946\n",
      "ag_news-erase Without Head 11 16 0.6827846\n",
      "ag_news-erase Without Head 11 32 0.662171\n",
      "ag_news-erase Without Head 11 64 0.65594393\n",
      "ag_news-erase Without Head 11 128 0.63299924\n",
      "ag_news-erase Without Head 11 256 0.6398547\n",
      "ag_news-erase Without Head 11 512 0.65171695\n",
      "ag_news-erase Without Head 7 16 0.6807822\n",
      "ag_news-erase Without Head 7 32 0.6148044\n",
      "ag_news-erase Without Head 7 64 0.6550175\n",
      "ag_news-erase Without Head 7 128 0.67127097\n",
      "ag_news-erase Without Head 7 256 0.64912045\n",
      "ag_news-erase Without Head 7 512 0.6506703\n",
      "ag_news-erase Head 13 16 0.86198103\n",
      "ag_news-erase Head 13 32 0.8943226\n",
      "ag_news-erase Head 13 64 0.91088486\n",
      "ag_news-erase Head 13 128 0.92450774\n",
      "ag_news-erase Head 13 256 0.925273\n",
      "ag_news-erase Head 13 512 0.93130946\n",
      "ag_news-erase Head 11 16 0.86617124\n",
      "ag_news-erase Head 11 32 0.89477706\n",
      "ag_news-erase Head 11 64 0.9158056\n",
      "ag_news-erase Head 11 128 0.91711986\n",
      "ag_news-erase Head 11 256 0.9255694\n",
      "ag_news-erase Head 11 512 0.9333389\n",
      "ag_news-erase Head 7 16 0.87013197\n",
      "ag_news-erase Head 7 32 0.89149624\n",
      "ag_news-erase Head 7 64 0.91662204\n",
      "ag_news-erase Head 7 128 0.9259806\n",
      "ag_news-erase Head 7 256 0.92472786\n",
      "ag_news-erase Head 7 512 0.9320492\n",
      "ag_news-replace Without Head 13 16 0.32230434\n",
      "ag_news-replace Without Head 13 32 0.28970602\n",
      "ag_news-replace Without Head 13 64 0.3194761\n",
      "ag_news-replace Without Head 13 128 0.3062203\n",
      "ag_news-replace Without Head 13 256 0.3199254\n",
      "ag_news-replace Without Head 13 512 0.3080308\n",
      "ag_news-replace Without Head 11 16 0.32226416\n",
      "ag_news-replace Without Head 11 32 0.2853236\n",
      "ag_news-replace Without Head 11 64 0.31568468\n",
      "ag_news-replace Without Head 11 128 0.3090292\n",
      "ag_news-replace Without Head 11 256 0.30611002\n",
      "ag_news-replace Without Head 11 512 0.30035335\n",
      "ag_news-replace Without Head 7 16 0.3209531\n",
      "ag_news-replace Without Head 7 32 0.3675287\n",
      "ag_news-replace Without Head 7 64 0.30311224\n",
      "ag_news-replace Without Head 7 128 0.29802865\n",
      "ag_news-replace Without Head 7 256 0.31011763\n",
      "ag_news-replace Without Head 7 512 0.29626855\n",
      "ag_news-replace Head 13 16 0.93805754\n",
      "ag_news-replace Head 13 32 0.95574975\n",
      "ag_news-replace Head 13 64 0.95955634\n",
      "ag_news-replace Head 13 128 0.96328974\n",
      "ag_news-replace Head 13 256 0.962719\n",
      "ag_news-replace Head 13 512 0.96368074\n",
      "ag_news-replace Head 11 16 0.93933964\n",
      "ag_news-replace Head 11 32 0.95231324\n",
      "ag_news-replace Head 11 64 0.9602895\n",
      "ag_news-replace Head 11 128 0.96412253\n",
      "ag_news-replace Head 11 256 0.963571\n",
      "ag_news-replace Head 11 512 0.96288466\n",
      "ag_news-replace Head 7 16 0.9399904\n",
      "ag_news-replace Head 7 32 0.9520399\n",
      "ag_news-replace Head 7 64 0.9599813\n",
      "ag_news-replace Head 7 128 0.96250224\n",
      "ag_news-replace Head 7 256 0.96397954\n",
      "ag_news-replace Head 7 512 0.9660967\n"
     ]
    }
   ],
   "source": [
    "upper_bound_collision = {}\n",
    "for dataset, f_d in features.items():\n",
    "    upper_bound_collision[dataset] = {}\n",
    "\n",
    "    for head_info, f_d_h in f_d.items():\n",
    "\n",
    "        upper_bound_collision[dataset][head_info] = {}\n",
    "        for seed, f_d_h_s in f_d_h.items():\n",
    "            negs = list(sorted(f_d_h_s))\n",
    "\n",
    "            for i, neg in enumerate(negs):\n",
    "                if neg not in upper_bound_collision[dataset][head_info]:\n",
    "                    upper_bound_collision[dataset][head_info][neg] = []\n",
    "\n",
    "                X_train_0, X_train_1, y_train = f_d_h[seed][neg]\n",
    "\n",
    "                C = len(np.unique(y_train))\n",
    "\n",
    "                X_train_0 = sklearn.preprocessing.normalize(X_train_0, axis=1)\n",
    "                X_train_1 = sklearn.preprocessing.normalize(X_train_1, axis=1)\n",
    "\n",
    "                upper_bounds = []\n",
    "\n",
    "                for c in range(C):\n",
    "                    upper_bounds.append(\n",
    "                        compute_bound(c, y_train, X_train_0, X_train_1)\n",
    "                    )\n",
    "\n",
    "                upper_bound = np.array(upper_bounds).flatten().mean()\n",
    "                print(dataset, head_info, seed, neg, upper_bound)\n",
    "                upper_bound_collision[dataset][head_info][neg].append(float(upper_bound))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"upper_bound_collision.json\", \"w\") as f:\n",
    "    json.dump(upper_bound_collision, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
